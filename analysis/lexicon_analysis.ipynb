{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-in module\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# 3rd-party module\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import tensorboard\n",
    "\n",
    "# self-made module\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter setting\n",
    "experiment_no = 30\n",
    "config = util.config.BaseConfig(# experiment no\n",
    "                                experiment_no=experiment_no,\n",
    "                                # preprocess\n",
    "                                tokenizer='WordPunctTokenizer',\n",
    "                                filter='punctonly',\n",
    "                                min_count=5,\n",
    "                                max_seq_len=20,\n",
    "                                # dataset and lexicon\n",
    "                                stance_dataset='semeval2016',\n",
    "                                embedding_file='glove/glove.twitter.27B.200d.txt',\n",
    "                                lexicon_file='emolex_emotion',\n",
    "                                # hyperparameter\n",
    "                                embedding_dim=200,\n",
    "                                task_hidden_dim=100,\n",
    "                                shared_hidden_dim=100,\n",
    "                                num_rnn_layers=1,\n",
    "                                num_linear_layers=1,\n",
    "                                attention='dot',\n",
    "                                dropout=0.2,\n",
    "                                learning_rate=1e-4,\n",
    "                                clip_grad_value=0,\n",
    "                                weight_decay=0,\n",
    "                                lr_decay_step=10,\n",
    "                                lr_decay=1,\n",
    "                                nli_loss_weight=1.0,\n",
    "                                lexicon_loss_weight=0,\n",
    "                                random_seed=77,\n",
    "                                kfold=5,\n",
    "                                train_test_split=0.15,\n",
    "                                epoch=50,\n",
    "                                batch_size=32)\n",
    "\n",
    "# initialize random seed and device\n",
    "device = torch.device('cpu')\n",
    "os.environ['PYTHONHASHSEED'] = str(config.random_seed)\n",
    "random.seed(config.random_seed)\n",
    "np.random.seed(config.random_seed)\n",
    "torch.manual_seed(config.random_seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    torch.cuda.manual_seed(config.random_seed)\n",
    "    # torch.cuda.manual_seed_all(config.random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "loading SemEval2016 training data: 100%|██████████| 2814/2814 [00:00<00:00, 1300580.88it/s]\n",
      "loading MultiNLI training data: 100%|██████████| 392702/392702 [00:01<00:00, 318210.61it/s]\n",
      "get all tokens: 100%|██████████| 522232/522232 [00:01<00:00, 398041.95it/s]\n",
      "load embedding: 100%|██████████| 1193514/1193514 [00:12<00:00, 94246.23it/s] \n",
      "content encode --\n",
      "loading EmoLex lexicon data: 100%|██████████| 141820/141820 [00:00<00:00, 1991743.52it/s]label encode --\n",
      "lexicon encode --\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "if config.stance_dataset == 'semeval2016':\n",
    "    stance_data_df = util.data.load_dataset('semeval2016_train')\n",
    "elif config.stance_dataset == 'fnc-1':\n",
    "    stance_data_df = util.data.load_dataset('fnc_train')\n",
    "\n",
    "nli_data_df = util.data.load_dataset('mnli_train')\n",
    "\n",
    "# initialize tokenizer\n",
    "if config.tokenizer == 'BaseTokenizer':\n",
    "    tokenizer = util.tokenizer.BaseTokenizer(config)\n",
    "elif config.tokenizer == 'WordPunctTokenizer':\n",
    "    tokenizer = util.tokenizer.WordPunctTokenizer(config)\n",
    "\n",
    "# initialize embedding\n",
    "if any([embedding in config.embedding_file for embedding in ['glove', 'fasttext']]):\n",
    "    embedding = util.embedding.BaseEmbedding(embedding_dim=config.embedding_dim)\n",
    "\n",
    "# get all tokens and embeddings\n",
    "all_sentence = []\n",
    "all_sentence.extend(stance_data_df['target'].drop_duplicates().tolist())\n",
    "all_sentence.extend(stance_data_df['claim'].drop_duplicates().tolist())\n",
    "all_sentence.extend(nli_data_df['target'].drop_duplicates().tolist())\n",
    "all_sentence.extend(nli_data_df['claim'].drop_duplicates().tolist())\n",
    "\n",
    "all_tokens = tokenizer.get_all_tokens(all_sentence)\n",
    "embedding.load_embedding(embedding_path=f'data/embedding/{config.embedding_file}',\n",
    "                        tokens=all_tokens)\n",
    "\n",
    "# build vocabulary dictionary\n",
    "tokenizer.build_dict(embedding.word_dict)\n",
    "\n",
    "# content encode to id\n",
    "print('content encode --')\n",
    "stance_data_df['target_encode'] = \\\n",
    "    tokenizer.encode(stance_data_df['target'].tolist())\n",
    "stance_data_df['claim_encode'] = \\\n",
    "    tokenizer.encode(stance_data_df['claim'].tolist())\n",
    "nli_data_df['target_encode'] = \\\n",
    "    tokenizer.encode(nli_data_df['target'].tolist())\n",
    "nli_data_df['claim_encode'] = \\\n",
    "    tokenizer.encode(nli_data_df['claim'].tolist())\n",
    "\n",
    "# label encode\n",
    "print('label encode --')\n",
    "if 'semeval' in config.stance_dataset:\n",
    "    stance_label = {'favor': 0, 'against': 1, 'none': 2}\n",
    "elif 'fnc' in config.stance_dataset:\n",
    "    stance_label = {'agree': 0, 'disagree': 1, 'discuss': 2, 'unrelated': 3}\n",
    "nli_label = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n",
    "\n",
    "stance_data_df['label_encode'] = stance_data_df['label'].apply(\n",
    "    lambda label: stance_label[label])\n",
    "nli_data_df['label_encode'] = nli_data_df['label'].apply(\n",
    "    lambda label: nli_label[label])\n",
    "\n",
    "# load lexicon\n",
    "lexicon = util.data.load_lexicon(lexicon=config.lexicon_file)\n",
    "\n",
    "# content encode to lexicon vector\n",
    "print('lexicon encode --')\n",
    "stance_data_df['claim_lexicon'] = \\\n",
    "    tokenizer.encode_to_lexicon(stance_data_df['claim_encode'].tolist(), lexicon)\n",
    "nli_data_df['claim_lexicon'] = \\\n",
    "    tokenizer.encode_to_lexicon(nli_data_df['claim_encode'].tolist(), lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_format_string(input_list):\n",
    "\n",
    "    for item in input_list:\n",
    "        print('{:<20}'.format(str(item)), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dear lord thank u for all of ur blessings forgive my sins lord give me strength and energy for this busy day ahead #blessed #hope #semst\n[bos]               dear                lord                thank               u                   for                 all                 of                  ur                  blessings           forgive             my                  sins                lord                give                me                  strength            and                 energy              [eos]               \n0                   0                   1                   0                   0                   0                   0                   0                   0                   1                   0                   0                   1                   1                   0                   0                   1                   0                   0                   0                   blessed are the peacemakers, for they shall be called children of god. matthew 5:9 #scripture #peace #semst\n[bos]               blessed             are                 the                 [unk]               for                 they                shall               be                  called              children            of                  god                 matthew             [unk]               [unk]               scripture           peace               [unk]               [eos]               \n0                   1                   0                   0                   0                   0                   0                   0                   0                   0                   1                   0                   1                   0                   0                   0                   1                   1                   0                   0                   i am not conformed to this world. i am transformed by the renewing of my mind. #ispeaklife #god #2014 #semst\n[bos]               i                   am                  not                 [unk]               to                  this                world               i                   am                  transformed         by                  the                 renewing            of                  my                  mind                [unk]               god                 [eos]               \n0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   1                   0                   salah should be prayed with #focus and #understanding. #allah warns against #lazy prayers done just for #show surah al-maoon 107:4-6 #semst\n[bos]               [unk]               should              be                  prayed              with                focus               and                 understanding       allah               warns               against             lazy                prayers             done                just                for                 show                [unk]               [eos]               \n0                   0                   0                   0                   1                   0                   0                   0                   1                   0                   1                   1                   0                   1                   1                   0                   0                   1                   0                   0                   and stay in your houses and do not display yourselves like that of the times of ignorance.\" [quran 33:33].#islam #semst\n[bos]               and                 stay                in                  your                houses              and                 do                  not                 display             yourselves          like                that                of                  the                 times               of                  ignorance           [unk]               [eos]               \n0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   1                   0                   0                   0                   0                   if we are unsure whether something is halal or haram, we should leave it - this will #safeguard our #deen #rule #semst\n[bos]               if                  we                  are                 unsure              whether             something           is                  [unk]               or                  haram               we                  should              leave               it                  this                will                safeguard           our                 [eos]               \n0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   1                   0                   0                   0                   1                   0                   0                   papa god, i pray that you shower me with more patience.  #worththewait #semst\n[bos]               papa                god                 i                   pray                that                you                 shower              me                  with                more                patience            [unk]               [unk]               [pad]               [pad]               [pad]               [pad]               [pad]               [eos]               \n0                   0                   1                   0                   1                   0                   0                   1                   0                   0                   0                   1                   0                   0                   0                   0                   0                   0                   0                   0                   now that the scoc has ruled canadians have freedom from religion, can someone tell harper to dummy his 'god bless canada'. #cdnpoli #semst\n[bos]               now                 that                the                 [unk]               has                 ruled               canadians           have                freedom             from                religion            can                 someone             tell                harper              to                  dummy               his                 [eos]               \n0                   0                   0                   0                   0                   0                   1                   0                   0                   1                   0                   1                   0                   0                   0                   0                   0                   0                   0                   0                   wow, unsubstantiated claims about spooks. remember whe i said there were gullible people? @jvx242 #semst\n[bos]               wow                 unsubstantiated     claims              about               [unk]               remember            [unk]               i                   said                there               were                gullible            people              [unk]               [unk]               [pad]               [pad]               [pad]               [eos]               \n0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   1                   0                   0                   0                   0                   0                   0                   0                   rt \"...that kind of modesty is too arrogant for me.\"í¢‘„‘îchristopher hitchens 2/2\" #funny #semst\n[bos]               [unk]               that                kind                of                  modesty             is                  too                 arrogant            for                 me                  [unk]               [unk]               [unk]               [unk]               hitchens            [unk]               [unk]               funny               [eos]               \n0                   0                   0                   1                   0                   1                   0                   0                   1                   0                   0                   0                   0                   0                   0                   1                   0                   0                   1                   0                   "
     ]
    }
   ],
   "source": [
    "temp_df = stance_data_df.iloc[:10]\n",
    "\n",
    "for _, line in temp_df.iterrows():\n",
    "    print(line['claim'])\n",
    "    print_format_string(tokenizer.convert_ids_to_tokens([line['claim_encode']])[0])\n",
    "    print()\n",
    "    print_format_string(line['claim_lexicon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}