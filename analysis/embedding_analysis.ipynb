{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.9 64-bit ('venv': venv)",
   "display_name": "Python 3.6.9 64-bit ('venv': venv)",
   "metadata": {
    "interpreter": {
     "hash": "3c30338ecb16d703437b097d5386462234541e53235eb04d52181597dd5d8e78"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## fastText and GloVe embedding\n",
    "\n",
    "## oov result\n",
    "\n",
    "| No | stance dataset | embedding file | num of out of vocabularys | percentage |\n",
    "| - | - | - | - | - |\n",
    "| 1 | semeval2016 | glove/glove.6B.50d.txt | 1508/7929 | 0.190 |\n",
    "| 2 | semeval2016 | glove/glove.twitter.27B.50d.txt | 1287/7929 | **0.162** |\n",
    "| 3 | semeval2016 | fasttext/wiki-news-300d-1M.vec | 1544/7929 | 0.194 |\n",
    "| 4 | semeval2016 | fasttext/crawl-300d-2M.vec | 1334/7929 | 0.168 |\n",
    "| 5 | fnc-1 | glove/glove.6B.50d.txt | 3460/24209 | **0.143** |\n",
    "| 6 | fnc-1 | glove/glove.twitter.27B.50d.txt | 4038/24209 | 0.167 |\n",
    "| 7 | fnc-1 | fasttext/wiki-news-300d-1M.vec | 4501/24209 | 0.186 |\n",
    "| 8 | fnc-1 | fasttext/crawl-300d-2M.vec | 3683/24209 | 0.152 |"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-in module\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# 3rd-party module\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "\n",
    "# self-made module\n",
    "import configs\n",
    "import datas\n",
    "import tokenizer\n",
    "import embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter setting\n",
    "embedding_no = 8\n",
    "config = configs.Config(stance_dataset='fnc-1',\n",
    "                        embedding_file='fasttext/crawl-300d-2M.vec',\n",
    "                        embedding_dim=300,\n",
    "                        random_seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define save path\n",
    "save_path = f'embedding/{embedding_no}'\n",
    "try:\n",
    "    os.makedirs(save_path)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize random seed\n",
    "os.environ['PYTHONHASHSEED'] = str(config.random_seed)\n",
    "random.seed(config.random_seed)\n",
    "np.random.seed(config.random_seed)\n",
    "_ = torch.manual_seed(config.random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "loading FNC-1 training data: 100%|██████████| 49972/49972 [00:11&lt;00:00, 4341.16it/s]\n"
    }
   ],
   "source": [
    "# load data\n",
    "if config.stance_dataset == 'semeval2016':\n",
    "    data_df = datas.load_dataset('semeval2016_train')\n",
    "elif config.stance_dataset == 'fnc-1':\n",
    "    data_df = datas.load_dataset('fnc_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "get all tokens: 100%|██████████| 3312/3312 [00:00&lt;00:00, 101038.90it/s]\nload embedding: 100%|█████████▉| 1999995/1999996 [00:21&lt;00:00, 94841.51it/s]\n"
    }
   ],
   "source": [
    "# initialize tokenizer and embedding\n",
    "tokenizers = tokenizer.Tokenizer()\n",
    "embedding = embeddings.Embedding(config.embedding_dim,\n",
    "                                 config.random_seed)\n",
    "\n",
    "# get all tokens and embeddings\n",
    "all_sentence = []\n",
    "all_sentence.extend(data_df['target'].drop_duplicates().tolist())\n",
    "all_sentence.extend(data_df['claim'].drop_duplicates().tolist())\n",
    "\n",
    "tokenizers.get_all_tokens(all_sentence)\n",
    "embedding.load_embedding(f'data/embedding/{config.embedding_file}',\n",
    "                         tokenizers.all_tokens)\n",
    "\n",
    "# build vocabulary dictionary\n",
    "tokenizers.build_dict(all_sentence, embedding.word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dataset: fnc-1\nembedding file: fasttext/crawl-300d-2M.vec\nnumber of oov: 3683 / 24209 (0.15213350406873477)\n"
    }
   ],
   "source": [
    "# get number of oov (out of vocabulary)\n",
    "token_to_id = tokenizers.token_to_id\n",
    "oov = [pair[0] for pair in token_to_id.items()\n",
    "       if pair[1] == tokenizers.unk_token_id]\n",
    "\n",
    "print(f'dataset: {config.stance_dataset}\\n'\n",
    "      f'embedding file: {config.embedding_file}\\n'\n",
    "      f'number of oov: {len(oov)} / {len(token_to_id)} ({len(oov)/len(token_to_id)})')"
   ]
  },
  {
   "source": [
    "## ELMo embedding\n",
    "\n",
    "## oov result"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-in module\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# 3rd-party module\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "\n",
    "# 3rd-party module for AllenNLP\n",
    "from allennlp.data import Token, Vocabulary\n",
    "from allennlp.data.fields import ListField, TextField\n",
    "from allennlp.data.token_indexers import (\n",
    "    SingleIdTokenIndexer,\n",
    "    TokenCharactersIndexer,\n",
    "    ELMoTokenCharactersIndexer,\n",
    ")\n",
    "from allennlp.data.tokenizers import (\n",
    "    SpacyTokenizer,\n",
    "    WhitespaceTokenizer,\n",
    ")\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import (\n",
    "    Embedding,\n",
    "    TokenCharactersEncoder,\n",
    "    ElmoTokenEmbedder,\n",
    ")\n",
    "from allennlp.nn import util as nn_util\n",
    "\n",
    "# self-made module\n",
    "import configs\n",
    "import datas\n",
    "import tokenizer\n",
    "import embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter setting\n",
    "embedding_no = 9\n",
    "config = configs.Config(stance_dataset='semeval',\n",
    "                        embedding_file='elmo/medium',\n",
    "                        embedding_dim=256,\n",
    "                        random_seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize random seed\n",
    "os.environ['PYTHONHASHSEED'] = str(config.random_seed)\n",
    "random.seed(config.random_seed)\n",
    "np.random.seed(config.random_seed)\n",
    "_ = torch.manual_seed(config.random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "if config.stance_dataset == 'semeval2016':\n",
    "    data_df = datas.load_dataset('semeval2016_train')\n",
    "elif config.stance_dataset == 'fnc-1':\n",
    "    data_df = datas.load_dataset('fnc_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define file path\n",
    "if 'medium' in config.embedding_file:\n",
    "    options_file = 'data/embedding/elmo/medium/elmo_2x2048_256_2048cnn_1xhighway_options.json'\n",
    "    weight_file = 'data/embedding/elmo/medium/elmo_2x2048_256_2048cnn_1xhighway_weights.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "# Indexer (token to id)\n",
    "token_indexer = ELMoTokenCharactersIndexer()\n",
    "\n",
    "# vocabulary\n",
    "vocab = Vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ELMo tokens: [Thissssscaac, is, some, text, .]\n"
    }
   ],
   "source": [
    "text = \"Thissssscaac is some text .\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"ELMo tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field = TextField(tokens, {'elmo_tokens': token_indexer})\n",
    "text_field.index(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ELMo tensors: {&#39;elmo_tokens&#39;: {&#39;elmo_tokens&#39;: tensor([[259,  85, 105, 106, 116, 116, 116, 116, 116, 100,  98,  98, 100, 260,\n         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n         261, 261, 261, 261, 261, 261, 261, 261],\n        [259, 106, 116, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n         261, 261, 261, 261, 261, 261, 261, 261],\n        [259, 116, 112, 110, 102, 260, 261, 261, 261, 261, 261, 261, 261, 261,\n         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n         261, 261, 261, 261, 261, 261, 261, 261],\n        [259, 117, 102, 121, 117, 260, 261, 261, 261, 261, 261, 261, 261, 261,\n         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n         261, 261, 261, 261, 261, 261, 261, 261],\n        [259,  47, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n         261, 261, 261, 261, 261, 261, 261, 261]])}}\n"
    }
   ],
   "source": [
    "token_tensor = text_field.as_tensor(padding_lengths)\n",
    "print(\"ELMo tensors:\", token_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_embedding = ElmoTokenEmbedder(options_file=options_file,\n",
    "                                   weight_file=weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = BasicTextFieldEmbedder(token_embedders={'elmo_tokens': elmo_embedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ELMo embedded tokens: tensor([[[ 0.0000,  0.0330,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n         [-0.0000, -0.2036,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n         [-0.2742,  0.0000, -0.6482,  ..., -0.0000,  0.1208,  0.0000],\n         [-0.0000, -0.0000, -0.0000,  ..., -0.5129,  0.0000, -0.8754],\n         [ 0.4370,  0.0000,  0.7828,  ...,  0.0000,  0.0000, -0.0000]]],\n       grad_fn=&lt;CatBackward&gt;)\n"
    }
   ],
   "source": [
    "tensor_dict = text_field.batch_tensors([token_tensor])\n",
    "embedded_tokens = embedder(tensor_dict)\n",
    "print(\"ELMo embedded tokens:\", embedded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([ 0.0000e+00,  3.3030e-02,  0.0000e+00, -2.6042e-01,  0.0000e+00,\n        -0.0000e+00, -2.9335e-01, -6.4144e-01,  0.0000e+00, -9.9162e-01,\n        -0.0000e+00, -7.2166e-01,  5.6376e-01,  0.0000e+00, -1.3957e-01,\n        -1.9405e-01, -1.8396e-01,  3.4711e-02,  6.2280e-01,  0.0000e+00,\n        -9.1091e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n        -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,\n         1.1339e-01, -1.1821e+00,  0.0000e+00, -0.0000e+00,  5.9862e-01,\n        -2.8574e-01, -9.8317e-01, -0.0000e+00, -1.2612e-01,  9.9614e-01,\n         6.5999e-01,  0.0000e+00,  3.0590e-01, -5.4797e-01, -5.0351e-01,\n         0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,\n        -8.8225e-01,  0.0000e+00, -1.0244e+00, -5.8098e-01,  3.0376e-01,\n         0.0000e+00, -0.0000e+00, -0.0000e+00, -1.1822e+00, -0.0000e+00,\n        -6.6956e-01, -0.0000e+00, -4.6374e-02, -7.3937e-02,  2.5804e-01,\n        -0.0000e+00, -1.8591e-01, -2.8674e-01, -2.8634e-01,  1.0081e+00,\n        -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,\n        -2.9123e-01,  0.0000e+00,  1.8401e-01,  0.0000e+00,  0.0000e+00,\n         0.0000e+00, -6.5904e-02,  8.4583e-01,  2.5280e-01,  0.0000e+00,\n         9.9426e-01, -2.2787e-01,  0.0000e+00,  3.7608e-01,  8.8086e-02,\n        -4.5794e+00, -0.0000e+00,  0.0000e+00,  9.3509e-01,  0.0000e+00,\n         5.7829e-01,  0.0000e+00, -8.3759e-01, -0.0000e+00,  0.0000e+00,\n         2.4131e-01, -0.0000e+00, -0.0000e+00, -0.0000e+00,  2.5045e-01,\n        -0.0000e+00,  0.0000e+00, -0.0000e+00, -3.1969e-01, -6.0930e-01,\n        -0.0000e+00,  0.0000e+00,  2.3375e-01,  0.0000e+00,  2.8950e-01,\n         3.6902e-01,  8.6540e-01, -1.0737e+00, -1.3925e+00, -0.0000e+00,\n        -3.3014e-01, -1.0709e+00,  3.4634e-02, -3.3597e-01,  0.0000e+00,\n         0.0000e+00, -9.3214e-01,  0.0000e+00, -0.0000e+00,  5.4437e-01,\n        -0.0000e+00, -9.7262e-01, -0.0000e+00,  2.7304e-01, -8.0681e-01,\n         1.1365e+00, -0.0000e+00,  1.0672e-01, -7.0580e-01,  0.0000e+00,\n        -3.6320e-01,  3.3650e-01,  0.0000e+00, -0.0000e+00, -0.0000e+00,\n         0.0000e+00,  3.7467e-02, -7.2124e-01,  0.0000e+00,  0.0000e+00,\n        -0.0000e+00, -0.0000e+00, -2.5992e-01,  1.1922e+00,  0.0000e+00,\n         0.0000e+00, -2.1142e-01, -5.7688e-01, -4.6651e-01, -2.7827e-01,\n        -3.5942e-01, -8.6523e-01, -0.0000e+00, -1.8504e-01,  1.0678e-02,\n        -0.0000e+00, -3.0471e-01,  1.5270e-01, -0.0000e+00,  0.0000e+00,\n        -6.2879e-02,  0.0000e+00,  1.9604e-01,  0.0000e+00, -0.0000e+00,\n        -9.7243e-02,  7.8086e-01,  1.3939e+00,  0.0000e+00,  0.0000e+00,\n         0.0000e+00, -0.0000e+00, -1.3258e-01,  0.0000e+00,  0.0000e+00,\n        -0.0000e+00, -5.0231e-01, -7.0191e-01, -3.2526e-01,  0.0000e+00,\n         6.8678e-01, -0.0000e+00,  6.0907e-01, -0.0000e+00, -0.0000e+00,\n         1.6411e+00,  1.3743e-01, -0.0000e+00,  0.0000e+00, -4.1671e-01,\n        -0.0000e+00, -1.7161e-01,  2.9144e-01, -3.8785e-01,  0.0000e+00,\n         0.0000e+00,  7.0012e-01,  0.0000e+00,  0.0000e+00,  1.2298e+00,\n        -0.0000e+00, -1.5383e-01, -3.0063e-01,  2.1923e+00,  0.0000e+00,\n        -6.0339e-01, -0.0000e+00,  1.1322e+00, -8.0949e-02, -0.0000e+00,\n        -0.0000e+00,  8.5599e-02,  0.0000e+00,  1.6627e+00,  9.0896e-01,\n        -1.7816e-01,  0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,\n        -2.4675e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.4780e-01,\n         4.4425e-01, -0.0000e+00,  0.0000e+00,  0.0000e+00, -2.2323e-01,\n        -0.0000e+00, -0.0000e+00,  3.0738e-01, -0.0000e+00, -0.0000e+00,\n         3.7138e-02, -3.9292e-01, -0.0000e+00, -0.0000e+00,  3.7823e-01,\n        -5.7990e-01,  5.1066e-01,  0.0000e+00,  2.9271e-01, -1.3132e+00,\n         2.2780e-01, -7.9222e-01, -1.8150e-01, -4.4970e-01,  1.7705e-01,\n         3.4584e-01, -0.0000e+00,  7.6835e-01, -0.0000e+00,  3.0689e-01,\n        -8.4311e-01, -0.0000e+00, -2.4369e-01,  1.0019e-01,  2.7239e-01,\n         0.0000e+00,  0.0000e+00,  5.6619e-01,  9.0561e-01,  6.3894e-01,\n         3.1854e-01, -0.0000e+00, -1.0644e-03, -2.4431e-01,  9.0403e-01,\n         0.0000e+00,  0.0000e+00,  1.7498e-01, -0.0000e+00, -0.0000e+00,\n         6.9817e-02,  8.4045e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n         1.3085e-01,  5.9065e-01, -7.3849e-01, -0.0000e+00,  0.0000e+00,\n         0.0000e+00,  4.2739e-01,  0.0000e+00, -8.8327e-01, -0.0000e+00,\n         5.7606e-01,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n        -0.0000e+00, -0.0000e+00,  6.8805e-01, -0.0000e+00,  2.6894e-01,\n        -0.0000e+00,  0.0000e+00,  1.0699e-01, -0.0000e+00,  4.1139e-01,\n         9.8541e-01, -4.7330e-01, -0.0000e+00,  4.1381e-03,  0.0000e+00,\n        -4.5894e-02, -0.0000e+00,  0.0000e+00, -7.3304e-01, -4.9120e-01,\n        -0.0000e+00, -1.9445e+00,  0.0000e+00, -2.4648e-01, -0.0000e+00,\n        -9.3506e-01, -0.0000e+00, -1.0030e+00,  0.0000e+00,  2.9206e-01,\n         0.0000e+00, -4.6199e-01, -0.0000e+00,  0.0000e+00,  6.7781e-01,\n        -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n        -7.8232e-02, -0.0000e+00,  8.0660e-01,  0.0000e+00,  0.0000e+00,\n         0.0000e+00,  9.4251e-02,  0.0000e+00, -4.9900e-01, -2.1609e-01,\n         8.2807e-01,  2.1222e-01,  6.3997e-01,  0.0000e+00,  0.0000e+00,\n         1.3375e+00, -0.0000e+00,  0.0000e+00,  9.7915e-02,  0.0000e+00,\n         2.9505e-01,  5.2410e-01,  0.0000e+00, -0.0000e+00,  2.3611e-02,\n         1.8044e+00, -7.7102e-01,  0.0000e+00, -0.0000e+00, -1.3130e-01,\n         0.0000e+00,  5.8056e-01,  0.0000e+00, -1.4055e+00,  0.0000e+00,\n        -1.4879e+00, -1.9763e-01, -0.0000e+00,  0.0000e+00,  0.0000e+00,\n        -0.0000e+00,  0.0000e+00, -1.8325e-01, -8.2281e-01,  1.2833e-01,\n         0.0000e+00,  0.0000e+00, -8.0480e-01,  1.7394e+00, -4.3657e-01,\n        -1.5228e-01,  3.3368e-01,  6.2277e-02,  0.0000e+00, -0.0000e+00,\n         3.8989e-01,  0.0000e+00,  7.5796e-01, -0.0000e+00,  3.5138e-01,\n         0.0000e+00, -3.8353e-01, -3.0182e-01,  0.0000e+00,  0.0000e+00,\n        -1.0162e+00,  1.0626e+00,  7.3293e-01, -0.0000e+00,  1.1514e+00,\n         1.0169e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00, -1.1039e+00,\n        -3.4699e-01,  0.0000e+00, -7.7400e-01,  0.0000e+00,  4.6387e-01,\n         8.6882e-01, -1.4467e-01, -0.0000e+00,  1.5282e-01,  1.9645e-01,\n         3.0829e-01,  2.6945e-01,  0.0000e+00, -3.2971e-01,  0.0000e+00,\n         3.6068e-01,  0.0000e+00,  0.0000e+00,  2.1521e-01, -0.0000e+00,\n         6.7348e-01,  7.7690e-01, -3.2530e-01, -0.0000e+00,  0.0000e+00,\n         0.0000e+00,  0.0000e+00, -2.1992e-01,  1.2571e+00, -4.5126e-01,\n         8.0813e-01,  9.7666e-01, -7.1682e-02,  1.3286e-01, -4.0148e-01,\n         8.6138e-01,  2.1554e-01,  1.0144e-01,  0.0000e+00, -0.0000e+00,\n        -1.4383e+00, -3.2418e-01, -3.0093e-01,  8.8628e-01, -0.0000e+00,\n         7.7401e-01, -0.0000e+00, -5.8688e-01, -5.3968e-01,  0.0000e+00,\n        -0.0000e+00, -0.0000e+00, -1.3323e+00,  0.0000e+00, -1.1786e+00,\n        -7.0341e-01, -0.0000e+00, -3.5264e-01,  1.2142e-01,  4.4281e-01,\n         0.0000e+00,  0.0000e+00,  9.9082e-02,  0.0000e+00, -1.8954e+00,\n        -3.2556e-01, -0.0000e+00,  0.0000e+00,  0.0000e+00,  1.3866e-01,\n         1.6947e-01, -0.0000e+00, -2.2650e-01, -0.0000e+00, -8.1017e-01,\n        -3.6858e-01, -1.1603e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,\n        -1.9899e-01, -2.2409e-02,  0.0000e+00,  7.1998e-02, -0.0000e+00,\n         5.0408e-01, -0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,\n        -0.0000e+00, -0.0000e+00], grad_fn=&lt;SelectBackward&gt;)"
     },
     "metadata": {},
     "execution_count": 110
    }
   ],
   "source": [
    "embedded_tokens[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}