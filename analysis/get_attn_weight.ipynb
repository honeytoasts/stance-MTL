{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('venv': venv)",
   "metadata": {
    "interpreter": {
     "hash": "3c30338ecb16d703437b097d5386462234541e53235eb04d52181597dd5d8e78"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-in module\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# 3rd-party module\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "\n",
    "# self-made module\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter and model path setting\n",
    "experiment_no = 7\n",
    "epoch = 21\n",
    "model_path = f'../model_v1.1/{experiment_no}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config, tokenizer, embedding\n",
    "config = util.config.BaseConfig()\n",
    "config = config.load(f'{model_path}/config.json')\n",
    "\n",
    "tokenizer = util.tokenizer.WordPunctTokenizer(config)\n",
    "tokenizer.load(f'{model_path}/tokenizer.pickle')\n",
    "\n",
    "embedding = util.embedding.BaseEmbedding()\n",
    "embedding.load(f'{model_path}/embedding.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize random seed and device\n",
    "device = torch.device('cpu')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_df = util.data.load_dataset_semeval2016_origin('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content encode to id\n",
    "data_df['claim_tokenize'] = \\\n",
    "    tokenizer.tokenize(data_df['claim_pre'].tolist())\n",
    "data_df['target_encode'] = \\\n",
    "    tokenizer.encode(data_df['target_pre'].tolist())\n",
    "data_df['claim_encode'] = \\\n",
    "    tokenizer.encode(data_df['claim_pre'].tolist())\n",
    "\n",
    "# content decode to token\n",
    "data_df['claim_decode'] = \\\n",
    "    tokenizer.convert_ids_to_tokens(data_df['claim_encode'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode\n",
    "stance_label = {'FAVOR': 0, 'AGAINST': 1, 'NONE': 2}\n",
    "data_df['label_encode'] = data_df['label'].apply(\n",
    "    lambda label: stance_label[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode content to lexicon vector\n",
    "data_df['claim_lexicon'] = \\\n",
    "    tokenizer.encode_to_lexicon(data_df['claim_encode'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset and dataloader\n",
    "dataset = util.data.SingleTaskDataset(\n",
    "    task_id=0,\n",
    "    target_encode=data_df['target_encode'],\n",
    "    claim_encode=data_df['claim_encode'],\n",
    "    claim_lexicon=data_df['claim_lexicon'],\n",
    "    label_encode=data_df['label_encode'])\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=util.data.SingleTaskDataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluate function\n",
    "def evaluate(model, batch_iterator):\n",
    "    all_label_y, all_pred_y = [], []\n",
    "    all_task_weight, all_shared_weight = [], []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for task_id, x1, x2, lexicon, y in batch_iterator:\n",
    "            # device\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            lexicon = lexicon.to(device)\n",
    "\n",
    "            # fed into model\n",
    "            pred_y, (task_weight, shared_weight) = model(task_id, x1, x2)\n",
    "\n",
    "            all_label_y.extend(y.tolist())\n",
    "            all_pred_y.extend(torch.argmax(pred_y, axis=1).cpu().tolist())\n",
    "            all_task_weight.extend(task_weight.tolist())\n",
    "            all_shared_weight.extend(shared_weight.tolist())\n",
    "\n",
    "    return (all_label_y, all_pred_y, \n",
    "            all_task_weight, all_shared_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = util.model.BaseModel(config=config,\n",
    "                             num_embeddings=embedding.get_num_embeddings(),\n",
    "                             padding_idx=tokenizer.pad_token_id,\n",
    "                             embedding_weight=embedding.vector)\n",
    "model.load_state_dict(\n",
    "    torch.load(f'{model_path}/model_{epoch}.ckpt'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "label, pred_label, task_weight, shared_weight = \\\n",
    "    evaluate(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert pred label into weight into dataframe\n",
    "data_df['label_pred'] = pred_label\n",
    "data_df['task_weight'] = task_weight\n",
    "data_df['shared_weight'] = shared_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out to csv\n",
    "data_path = '../data/attn_weight'\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "data_df.to_csv(f'{data_path}/v1.1_test_weight.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}